# -*- coding: utf-8 -*-
"""Projeto Final.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1oNpG2btgBzhWyTrM6iOiNMWO7TsAofj8

1. Obtenção dos Dados
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Para fazer download dos dados direto do computador use os seguintes comandos:
from google.colab import files
uploaded = files.upload()

# carregar dados no formato CSV (dentro do Drive)
df_dados = pd.read_csv('bootcamp_train - bootcamp_train.csv')
type(df_dados)

"""2. Entendeminto da Estrutura do Dataset"""

df_dados.head(10)

df_dados.tail(10)

df_dados.info()

df_dados.espessura_da_chapa_de_aço.value_counts()

df_dados.tipo_do_aço_A300.value_counts()

df_dados.tipo_do_aço_A400.value_counts()

"""3. Tratamento de Dados, Erros, Ausentes, Formato de dados"""

#Transformando binario os dados nas colunas "tipo_do_aço_A300"
# substituição nas colunas "tipo_do_aço_A300"
# Convertir a tipo numérico (por si existem strings)
valores_0 = ["Não", "não", "N", "nao", "-", 0]
valores_1 = ["Sim", "sim", "S", 1]

df_dados["tipo_do_aço_A300"] = df_dados["tipo_do_aço_A300"].replace(valores_0, 0)
df_dados["tipo_do_aço_A300"] = df_dados["tipo_do_aço_A300"].replace(valores_1, 1)

df_dados["tipo_do_aço_A300"] = pd.to_numeric(df_dados["tipo_do_aço_A300"], errors='coerce')

df_dados.tipo_do_aço_A300.value_counts()

df_dados.tipo_do_aço_A300.info()

#Transformando binario os dados nas colunas "tipo_do_aço_A400"
# substituição nas colunas "tipo_do_aço_A400"
# Convertir a tipo numérico (por si existem strings)
df_dados["tipo_do_aço_A400"] = df_dados["tipo_do_aço_A400"].replace(valores_0, 0)
df_dados["tipo_do_aço_A400"] = df_dados["tipo_do_aço_A400"].replace(valores_1, 1)

df_dados["tipo_do_aço_A400"] = pd.to_numeric(df_dados["tipo_do_aço_A400"], errors='coerce')

df_dados.tipo_do_aço_A400.value_counts()

df_dados.tipo_do_aço_A400.info()

# Garantir que valores vazios sejam tratados como NaN
# Preencher os NaNs de A400 com o oposto binário de "tipo_do_aço_A300"
df_dados['tipo_do_aço_A400'] = pd.to_numeric(df_dados['tipo_do_aço_A400'], errors='coerce')

df_dados.loc[df_dados['tipo_do_aço_A400'].isna(), 'tipo_do_aço_A400'] = 1 - df_dados.loc[df_dados['tipo_do_aço_A400'].isna(), 'tipo_do_aço_A300']

df_dados.tipo_do_aço_A400.value_counts()

df_dados.tipo_do_aço_A400.info()

# Converter a coluna 'tipo_do_aço_A400' de float para int
df_dados['tipo_do_aço_A400'] = df_dados['tipo_do_aço_A400'].astype(int)

df_dados.tipo_do_aço_A400.info()

#removendo os valores -154.
#E substituindo os valores vazios por valores aleatórios baseados em valores típicos relacionados a 'type_of_aço_A300', 'type_of_aço_A400'
df_dados['espessura_da_chapa_de_aço'] = df_dados['espessura_da_chapa_de_aço'].replace(-154, np.nan)

# funçao substituiçao aleatorio por grupo
def completar_aleatoriamente(grupo):
    valores_validos = grupo['espessura_da_chapa_de_aço'].dropna().values
    nulos = grupo['espessura_da_chapa_de_aço'].isna()
    if len(valores_validos) > 0:
        grupo.loc[nulos, 'espessura_da_chapa_de_aço'] = np.random.choice(valores_validos, size=nulos.sum())
    return grupo

# Agrupar pelas combinações de tipo_de_aço_A300 e tipo_de_aço_A400
df_dados = df_dados.groupby(['tipo_do_aço_A300', 'tipo_do_aço_A400'], group_keys=False).apply(completar_aleatoriamente)

df_dados.espessura_da_chapa_de_aço.value_counts()

df_dados.espessura_da_chapa_de_aço.info()

#index_quadrado apresenta 125 valores > 10, nao e um indice, e apresenta 43 null, no existe como correlacionar eses dados.Serão excluídas filas do dataset
df_dados.index_quadrado.info()

#index_quadrado apresenta 125 valores > 10, nenhum é um índice, e apresenta 43 nulos. Essas linhas do conjunto de dados serão excluídas
# Excluir linhas com valor 125 ou NaN na coluna 'index_quadrado'
df_dados = df_dados[~((df_dados['index_quadrado'] == 125) | (df_dados['index_quadrado'].isna()))]

df_dados.log_indice_x.info()

#log_indice_x apresenta no inicio do dataset 32 valores 699, que se repete aleatoriamente.Serão excluídos
# Excluir linhas onde 'log_index_x' é 699 ou NaN
df_dados = df_dados[~((df_dados['log_indice_x'] == 699) | (df_dados['log_indice_x'].isna()))]

df_dados.log_indice_x.info()

#log_indice_x apresenta no inicio do dataset 35 valores 699, que se repete aleatoriamente.Serão excluidos
# Excluir linhas onde 'log_index_y' é 699 ou NaN
df_dados = df_dados[~((df_dados['log_indice_y'] == 699) | (df_dados['log_indice_y'].isna()))]

df_dados.log_indice_y.info()

df_dados = df_dados[~df_dados['indice_de_orientaçao'].isna()]

df_dados.indice_de_orientaçao.info()

# Excluir linhas onde 'soma_da_luminosidade' NaN
df_dados = df_dados[~df_dados['soma_da_luminosidade'].isna()]

df_dados.soma_da_luminosidade.info()

#substituição o formato das colunas de falhas para um formato binário de 0 e 1
df_dados.falha_1.value_counts()

df_dados.falha_2.value_counts()

df_dados.falha_3.value_counts()

df_dados.falha_4.value_counts()

df_dados.falha_5.value_counts()

df_dados.falha_6.value_counts()

df_dados.falha_outros.value_counts()

colunas_falhas = ["falha_1", "falha_2", "falha_3", "falha_4", "falha_5", "falha_6", "falha_outros"]

# valores normalizados representando falso e verdadeiro
valores_falsos = {"false", "0", "não", "nao", "n"}
valores_certos = {"true", "1", "sim", "s", "y"}

for coluna in colunas_falhas:

    normalizado = df_dados[coluna].astype(str).str.strip().str.lower()

    df_dados[coluna] = np.nan

    df_dados.loc[normalizado.isin(valores_falsos), coluna] = 0

    df_dados.loc[normalizado.isin(valores_certos), coluna] = 1

    df_dados[coluna] = df_dados[coluna].astype('Int64')  # usa 'Int64' para permitir NaN si hay

print(df_dados[colunas_falhas])

#valores booleanos e contá-los
conteo_booleanos = df_dados[colunas_falhas].apply(
    lambda col: col.map(type).eq(bool).sum()
)

print("Cantidad de valores booleanos por coluna:")
print(conteo_booleanos)

# Contar cantidade de falhas
conteo_por_coluna = df_dados[colunas_falhas].sum()

total_falhas = conteo_por_coluna.sum()

print("Cantidade de falhas :")
print(conteo_por_coluna)

print(f"\nCantidade total de falhas (1s): {total_falhas}")

# Colunas a desconsiderar por que tem muitos datos vazios, erros, informaçao fora do padrao etc. Eliminar (limpada y ajustada)
colunas_a_eliminar = [
    "x_minimo", "x_maximo", "y_minimo", "y_maximo", "peso_da_placa", "area_pixels",
    "perimetro_x", "perimetro_y", "maximo_da_luminosidade", "comprimento_do_transportador",
    "temperatura", "index_de_bordas", "index_vazio", "index_externo_x", "indice_de_bordas_x",
    "indice_de_bordas_y", "indice_de_variacao_x", "indice_de_variacao_y", "indice_global_externo",
    "log_das_areas", "indice_de_luminosidade", "sigmoide_das_areas", "minimo_da_luminosidade"
]

# Eliminar colunas do DataFrame
# Criar df_filtrado
colunas_existentes = [col for col in colunas_a_eliminar if col in df_dados.columns]

df_filtrado = df_dados.drop(columns=colunas_existentes)

#df_filtrado e o dataframe reduzido para tratar
df_filtrado.info()

df_filtrado

# quantidade falhas no dataframe df_filtrado
conteio_por_coluna = df_filtrado[colunas_falhas].sum()

total_falhas = conteio_por_coluna.sum()

print("Cantidade de 1 por coluna:")
print(conteio_por_coluna)

print(f"\nCantidad total de falhas (1s): {total_falhas}")

#plotando soma_da_luminosidade para cada falha
plt.figure(figsize=(14, 10))

for i, col in enumerate(colunas_falhas, 1):
    plt.subplot(3, 3, i)

    dados_filtrados = df_filtrado[df_filtrado[col] == 1]

    plt.scatter([col] * len(dados_filtrados), dados_filtrados['soma_da_luminosidade'],
                alpha=0.5, color='orangered')

    plt.title(f'{col}')
    plt.ylabel('soma_da_luminosidade')
    plt.xticks([])

plt.tight_layout()
plt.suptitle('soma_da_luminosidade para casos com falha = 1', fontsize=16, y=1.02)
plt.show()

#existem valores muito grandes, são incoerentes (parecer técnico), podem afetar o desempenho de modelos.
#Serão excluidos
# Eliminar linhas com falha_2 == 1 e soma_da_luminosidade > 0.6e6
df_filtrado = df_filtrado[~((df_filtrado['falha_2'] == 1) & (df_filtrado['soma_da_luminosidade'] > 0.4e6))]

# Eliminar linhas com falha_3 == 1 e soma_da_luminosidade > 0.4e7
df_filtrado = df_filtrado[~((df_filtrado['falha_3'] == 1) & (df_filtrado['soma_da_luminosidade'] > 2.8e6))]

# Eliminar linhas com falha_4 == 1 e soma_da_luminosidade > 8_000
df_filtrado = df_filtrado[~((df_filtrado['falha_4'] == 1) & (df_filtrado['soma_da_luminosidade'] > 7_000))]

# Eliminar linhas com falha_5 == 1 e soma_da_luminosidade > 200_000
df_filtrado = df_filtrado[~((df_filtrado['falha_5'] == 1) & (df_filtrado['soma_da_luminosidade'] > 0.15e6))]

# Eliminar linhas com falha_6 == 1 e soma_da_luminosidade > 0.8e6
df_filtrado = df_filtrado[~((df_filtrado['falha_6'] == 1) & (df_filtrado['soma_da_luminosidade'] > 0.8e6))]

# Eliminar linhas com falha_outros == 1 e soma_da_luminosidade > 3e6
df_filtrado = df_filtrado[~((df_filtrado['falha_outros'] == 1) & (df_filtrado['soma_da_luminosidade'] > 2.5e6))]

plt.figure(figsize=(14, 10))

for i, col in enumerate(colunas_falhas, 1):
    plt.subplot(3, 3, i)

    dados_filtrados = df_filtrado[df_filtrado[col] == 1]

    plt.scatter([col] * len(dados_filtrados), dados_filtrados['soma_da_luminosidade'],
                alpha=0.5, color='orangered')

    plt.title(f'{col} ')
    plt.ylabel('soma_da_luminosidade')
    plt.xticks([])

plt.tight_layout()
plt.suptitle('soma_da_luminosidade para casos com falha = 1', fontsize=16, y=1.02)
plt.show()

df_filtrado.info()

# quantidade de falhas por coluna
conteio_por_coluna = df_filtrado[colunas_falhas].sum()
total_falhas = conteio_por_coluna.sum()
print("Cantidade de 1 por coluna:")
print(conteio_por_coluna)

print(f"\nCantidad total de falhas (1s): {total_falhas}")

#Plotando boxplot de soma de luminosidade para cada falha
#Boxplot falha_1
falha1_ativa = df_filtrado[df_filtrado["falha_1"] == 1]

plt.figure(figsize=(6, 6))
sns.boxplot(y=falha1_ativa['soma_da_luminosidade'], color='lightcoral')

plt.title("Distribuição de soma_da_luminosidade (falha_1)", fontsize=14)
plt.ylabel("soma_da_luminosidade")
plt.xticks([])
plt.grid(axis='y', linestyle='--', alpha=0.4)
plt.tight_layout()
plt.show()

#Boxplot falha_2
falha2_ativa = df_filtrado[df_filtrado["falha_2"] == 1]

plt.figure(figsize=(6, 5))
sns.boxplot(y=falha2_ativa['soma_da_luminosidade'], color='mediumseagreen')

plt.title("Distribuição de soma_da_luminosidade (falha_2)", fontsize=14)
plt.ylabel("soma_da_luminosidade")
plt.xticks([])
plt.grid(axis='y', linestyle='--', alpha=0.4)
plt.tight_layout()
plt.show()

#Boxplot falha_3
falha3_ativa = df_filtrado[df_filtrado["falha_3"] == 1]

plt.figure(figsize=(6, 5))
sns.boxplot(y=falha3_ativa['soma_da_luminosidade'], color='goldenrod')

plt.title("Distribuição de soma_da_luminosidade (falha_3)", fontsize=14)
plt.ylabel("soma_da_luminosidade")
plt.xticks([])
plt.grid(axis='y', linestyle='--', alpha=0.4)
plt.tight_layout()
plt.show()

#Boxplot falha_4
falha4_ativa = df_filtrado[df_filtrado["falha_4"] == 1]

plt.figure(figsize=(6, 5))
sns.boxplot(y=falha4_ativa['soma_da_luminosidade'], color='slateblue')

plt.title("Distribuição de soma_da_luminosidade (falha_4)", fontsize=14)
plt.ylabel("soma_da_luminosidade")
plt.xticks([])
plt.grid(axis='y', linestyle='--', alpha=0.4)
plt.tight_layout()
plt.show()

#Boxplot falha_5
falha5_ativa = df_filtrado[df_filtrado["falha_5"] == 1]

plt.figure(figsize=(6, 5))
sns.boxplot(y=falha5_ativa['soma_da_luminosidade'], color='tomato')

plt.title("Distribuição de soma_da_luminosidade (falha_5)", fontsize=14)
plt.ylabel("soma_da_luminosidade")
plt.xticks([])
plt.grid(axis='y', linestyle='--', alpha=0.4)
plt.tight_layout()
plt.show()

#Boxplot falha_6
falha6_ativa = df_filtrado[df_filtrado["falha_6"] == 1]

plt.figure(figsize=(6, 5))
sns.boxplot(y=falha6_ativa['soma_da_luminosidade'], color='mediumorchid')

plt.title("Distribuição de soma_da_luminosidade (falha_6)", fontsize=14)
plt.ylabel("soma_da_luminosidade")
plt.xticks([])
plt.grid(axis='y', linestyle='--', alpha=0.4)
plt.tight_layout()
plt.show()

#Boxplot falha_outros
falha_outros_ativa = df_filtrado[df_filtrado["falha_outros"] == 1]

plt.figure(figsize=(6, 5))
sns.boxplot(y=falha_outros_ativa['soma_da_luminosidade'], color='darkcyan')

plt.title("Distribuição de soma_da_luminosidade (falha_outros)", fontsize=14)
plt.ylabel("soma_da_luminosidade")
plt.xticks([])
plt.grid(axis='y', linestyle='--', alpha=0.4)
plt.tight_layout()
plt.show()

#plotando disperçao da espessura_da_chapa_de_aço para cada falha
plt.figure(figsize=(14, 10))

for i, col in enumerate(colunas_falhas, 1):
    plt.subplot(3, 3, i)

    # Filtrar apenas onde a falha = 1
    dados_filtrados = df_filtrado[df_filtrado[col] == 1]

    plt.scatter([col] * len(dados_filtrados), dados_filtrados['espessura_da_chapa_de_aço'],
                alpha=0.5, color='teal')

    plt.title(f'{col} ')
    plt.ylabel('espessura_da_chapa_de_aço')
    plt.xticks([])

plt.tight_layout(rect=[0, 0, 1, 0.96])
plt.suptitle('espessura_da_chapa_de_aço para casos com falha', fontsize=16)
plt.show()

#plotando grafico de barras da espessura_da_chapa_de_aço para cada falha

for col in colunas_falhas:
    # Filtrar os dados onde a falha atual é 1
    dados_falha = df_filtrado[df_filtrado[col] == 1]

    # Contar quantas vezes cada espessura aparece
    contagem_espessura = dados_falha['espessura_da_chapa_de_aço'] \
        .value_counts() \
        .sort_index()

    plt.figure(figsize=(10, 5))
    bars = plt.bar(
        contagem_espessura.index,
        contagem_espessura.values,
        color='teal',
        alpha=0.7, width=1
    )

    # Anotar a quantidade acima de cada barra
    for bar in bars:
        altura = bar.get_height()
        x_pos = bar.get_x() + bar.get_width() / 2
        plt.text(
            x_pos, altura + 0.5,
            int(altura),
            ha='center', va='bottom'
        )

    plt.xlabel('Espessura da chapa de aço')
    plt.ylabel(f'Número de ocorrências ({col})')
    plt.title(f'Frequência das espessuras para {col}')
    plt.tight_layout()
    plt.show()

for col in colunas_falhas:
    # Filtrar linhas onde a falha atual ocorreu
    dados_falha = df_filtrado[df_filtrado[col] == 1]

    # Contar quantos casos têm cada tipo de aço (assumindo one-hot encoding)
    qtd_a300 = dados_falha['tipo_do_aço_A300'].sum()
    qtd_a400 = dados_falha['tipo_do_aço_A400'].sum()

    # Dados para plotagem
    tipos = ['A300', 'A400']
    quantidades = [qtd_a300, qtd_a400]

    plt.figure(figsize=(4, 3))
    bars = plt.bar(tipos, quantidades, color='darkorange', alpha=0.4, width=0.4)

    # Anotar valor acima das barras
    for bar in bars:
        altura = bar.get_height()
        plt.text(bar.get_x() + bar.get_width() / 2, altura + 0.5,
                 int(altura), ha='center', va='bottom')

    plt.title(f'{col} - Contagem por tipo de aço')
    plt.xlabel('Tipo de aço')
    plt.ylabel('Número de ocorrências')
    plt.ylim(0, max(quantidades) * 1.2)
    plt.tight_layout()
    plt.show()

# Plotando quantidade de falhas por tipo
contagem_falhas = df_filtrado[colunas_falhas].sum()


plt.figure(figsize=(10, 6))
barras = plt.bar(colunas_falhas, contagem_falhas, color='skyblue')

# valores no topo das barras
for barra in barras:
    altura = barra.get_height()
    plt.text(barra.get_x() + barra.get_width() / 2, altura + 10, f'{int(altura)}',
             ha='center', va='bottom', fontsize=10)

# Títulos e eixos
plt.title("Quantidade de Falhas por Tipo", fontsize=14)
plt.xlabel("Tipo de Falha")
plt.ylabel("Quantidade falhas)")
plt.grid(axis='y', linestyle='--', alpha=0.6)

plt.tight_layout()
plt.show()

"""#As salidas estão desbalanceadas para modelagem"""

#plotando dispersão 'log_indice_x' para cada falha
plt.figure(figsize=(14, 10))

for i, col in enumerate(colunas_falhas, 1):
    plt.subplot(3, 3, i)

    dados_filtrados = df_filtrado[df_filtrado[col] == 1]


    plt.scatter([col] * len(dados_filtrados), dados_filtrados['log_indice_x'],
                alpha=0.5, color='teal')

    plt.title(f'{col}')
    plt.ylabel('log_indice_x')
    plt.xticks([])

plt.tight_layout(rect=[0, 0, 1, 0.96])
plt.suptitle('log_indice_x para casos com falha = 1', fontsize=16)
plt.show()

#existem valores muito grandes, são incoerentes (parecer técnico), podem afetar o desempenho de modelos.
#Serão excluidos
# Eliminar linhas com falha_3 == 1 e log_indice_x > 50
df_filtrado = df_filtrado[~((df_filtrado['falha_3'] == 1) & (df_filtrado['log_indice_x'] > 50))]

# Eliminar linhas com falha_4 == 1 e log_indice_x > 50
df_filtrado = df_filtrado[~((df_filtrado['falha_4'] == 1) & (df_filtrado['log_indice_x'] > 50))]

#plotando dispersão 'log_indice_y' para cada falha
plt.figure(figsize=(14, 10))

for i, col in enumerate(colunas_falhas, 1):
    plt.subplot(3, 3, i)


    dados_filtrados = df_filtrado[df_filtrado[col] == 1]


    plt.scatter([col] * len(dados_filtrados), dados_filtrados['log_indice_x'],
                alpha=0.5, color='teal')

    plt.title(f'{col} ')
    plt.ylabel('log_indice_x')
    plt.xticks([])

plt.tight_layout(rect=[0, 0, 1, 0.96])
plt.suptitle('log_indice_x para casos com falha = 1', fontsize=16)
plt.show()

#plotando dispersão 'log_indice_y' para cada falha
plt.figure(figsize=(14, 10))

for i, col in enumerate(colunas_falhas, 1):
    plt.subplot(3, 3, i)

    dados_filtrados = df_filtrado[df_filtrado[col] == 1]

    plt.scatter([col] * len(dados_filtrados), dados_filtrados['log_indice_y'],
                alpha=0.5, color='teal')

    plt.title(f'{col} ')
    plt.ylabel('log_indice_y')
    plt.xticks([])

plt.tight_layout(rect=[0, 0, 1, 0.96])
plt.suptitle('log_indice_y para cada tipo de falha', fontsize=16)
plt.show()

#neste casso não tinha detetado o dado 300 nas anteriores analises. Este dado se repete e esta fora da ordem de grandeza. Serão excluido
# Eliminar linhas com falha_4 == 1 e log_indice_y > 50
df_filtrado = df_filtrado[~((df_filtrado['falha_4'] == 1) & (df_filtrado['log_indice_y'] > 50))]
# Eliminar linhas com falha_6 == 1 e log_indice_y > 50
df_filtrado = df_filtrado[~((df_filtrado['falha_6'] == 1) & (df_filtrado['log_indice_y'] > 50))]
# Eliminar linhas com falha_outros == 1 e log_indice_y > 50
df_filtrado = df_filtrado[~((df_filtrado['falha_outros'] == 1) & (df_filtrado['log_indice_y'] > 50))]

#plotando dispersão 'log_indice_y' para cada falha

plt.figure(figsize=(14, 10))

for i, col in enumerate(colunas_falhas, 1):
    plt.subplot(3, 3, i)

    # Filtrar apenas onde a falha = 1
    dados_filtrados = df_filtrado[df_filtrado[col] == 1]

    # Scatter plot vertical para cada falha
    plt.scatter([col] * len(dados_filtrados), dados_filtrados['log_indice_y'],
                alpha=0.5, color='teal')

    plt.title(f'{col} ')
    plt.ylabel('log_indice_y')
    plt.xticks([])

plt.tight_layout(rect=[0, 0, 1, 0.96])
plt.suptitle('log_indice_y para cada casos de falha', fontsize=16)
plt.show()

#plotando dispersão 'indice_de_orientaçao' para cada falha

plt.figure(figsize=(14, 10))

for i, col in enumerate(colunas_falhas, 1):
    plt.subplot(3, 3, i)

    dados_filtrados = df_filtrado[df_filtrado[col] == 1]

    plt.scatter([col] * len(dados_filtrados), dados_filtrados['indice_de_orientaçao'],
                alpha=0.5, color='teal')

    plt.title(f'{col} ')
    plt.ylabel('indice_de_orientaçao')
    plt.xticks([])

plt.tight_layout(rect=[0, 0, 1, 0.96])
plt.suptitle('indice_de_orientaçao para cada falha', fontsize=16)
plt.show()

# Eliminar linhas com falha_1 == 1 e indice_de_orientaçao < -250
df_filtrado = df_filtrado[~((df_filtrado['falha_1'] == 1) & (df_filtrado['indice_de_orientaçao'] < -250))]

# Eliminar linhas com falha_4 == 1 e indice_de_orientaçao < -20
df_filtrado = df_filtrado[~((df_filtrado['falha_4'] == 1) & (df_filtrado['indice_de_orientaçao'] < -20))]
# Eliminar linhas com falha_5 == 1 e indice_de_orientaçao > 100
df_filtrado = df_filtrado[~((df_filtrado['falha_5'] == 1) & (df_filtrado['indice_de_orientaçao'] > 100))]

#plotando dispersão 'indice_de_orientaçao' para cada falha

plt.figure(figsize=(14, 10))

for i, col in enumerate(colunas_falhas, 1):
    plt.subplot(3, 3, i)

    # Filtrar apenas onde a falha = 1
    dados_filtrados = df_filtrado[df_filtrado[col] == 1]

    # Scatter plot vertical para cada falha
    plt.scatter([col] * len(dados_filtrados), dados_filtrados['indice_de_orientaçao'],
                alpha=0.5, color='teal')

    plt.title(f'{col}')
    plt.ylabel('indice_de_orientaçao')
    plt.xticks([])

plt.tight_layout(rect=[0, 0, 1, 0.96])
plt.suptitle('indice_de_orientaçao para cada falha', fontsize=16)
plt.show()

df_filtrado.info()

# Contar cantidade de falhas por coluna
conteio_por_coluna = df_filtrado[colunas_falhas].sum()
total_falhas = conteio_por_coluna.sum()
print("Cantidade de 1 por coluna:")
print(conteio_por_coluna)

print(f"\nCantidad total de falhas (1s): {total_falhas}")

#plotando soma_da_luminosidade
plt.figure(figsize=(10, 4))
plt.scatter(df_filtrado.index, df_filtrado['soma_da_luminosidade'], alpha=0.6)
plt.title('Dispersão da Soma da Luminosidade')
plt.xlabel('Índice')
plt.ylabel('Soma da Luminosidade')
plt.grid(True)
plt.show()

# Aplicando log(1 + x) para reduzir comprimir valores muito altos e expandir os baixos.
df_filtrado['log_soma_da_luminosidade'] = np.log1p(df_filtrado['soma_da_luminosidade'])
plt.figure(figsize=(10, 4))
plt.scatter(df_filtrado.index, df_filtrado['log_soma_da_luminosidade'], alpha=0.6)
plt.title('Dispersão da log_soma_da_luminosidade')
plt.xlabel('Índice')
plt.ylabel('log_soma_da_luminosidade')
plt.grid(True)
plt.show()
#Como são diferentes tipos de laminas, essa variável provavelmente carrega informação importante, apresentando varias tendencias.Como o tratamento é mais complicado.
# vou tratar os dados só com log_suma_da_luminosidade. Para usar o modelo de aprendizado Random Forest

#plotando espessura_da_chapa_de_aço
plt.figure(figsize=(10, 4))
plt.scatter(df_filtrado.index, df_filtrado['espessura_da_chapa_de_aço'], alpha=0.6)
plt.title('espessura_da_chapa_de_aço')
plt.xlabel('Índice')
plt.ylabel('espessura_da_chapa_de_aço')
plt.grid(True)
plt.show()
# espessura_da_chapa_de_aço não é uma variavel discreta com valores específicos e repetidos, é totalmente esperado para materiais padronizados.
#vou tratar como está numérica discreta, usando modelo RandomForest.

#Plotando index_quadrado
plt.figure(figsize=(10, 4))
plt.scatter(df_filtrado.index, df_filtrado['index_quadrado'], alpha=0.6)
plt.title('index_quadrado')
plt.xlabel('Índice')
plt.ylabel('index_quadrado')
plt.grid(True)
plt.show()

# Aplicando log(1 + x) para reduzir comprimir valores muito altos e expandir os baixos.
df_filtrado["log_index_quadrado"] = np.log1p(df_filtrado["index_quadrado"])
df_filtrado = df_filtrado[(df_filtrado["log_index_quadrado"] < 3) | (df_filtrado["log_index_quadrado"] > 5)]
plt.figure(figsize=(10, 4))
plt.scatter(df_filtrado.index, df_filtrado['log_index_quadrado'], alpha=0.6)
plt.title('log_index_quadrado')
plt.xlabel('Índice')
plt.ylabel('log_index_quadrado')
plt.grid(True)
plt.show()
# cada faixa pode corresponder a uma família de produtos com características geométricas distintas.
#Poderia treinar modelos separados para cada faixa.

#Plotando log_indice_x

plt.figure(figsize=(10, 4))
plt.scatter(df_filtrado.index, df_filtrado['log_indice_x'], alpha=0.6)
plt.title('log_indice_x')
plt.xlabel('Índice')
plt.ylabel('log_indice_x')
plt.grid(True)
plt.show()

#Plotando log_indice_y

plt.figure(figsize=(10, 4))
plt.scatter(df_filtrado.index, df_filtrado['log_indice_y'], alpha=0.6)
plt.title('log_indice_y')
plt.xlabel('Índice')
plt.ylabel('log_indice_y')
plt.grid(True)
plt.show()

#plotando indice_de_orientaçao
# Aplicando log(1 + x) para reduzir comprimir valores muito altos e expandir os baixos.
df_filtrado["log_indice_de_orientaçao"] = np.log1p(df_filtrado["indice_de_orientaçao"])

plt.figure(figsize=(10, 4))
plt.scatter(df_filtrado.index, df_filtrado['log_indice_de_orientaçao'], alpha=0.6)
plt.title('log_indice_de_orientaçao')
plt.xlabel('Índice')
plt.ylabel('log_indice_de_orientaçao')
plt.grid(True)
plt.show()
#Poderia treinar modelos separados para cada faixa.

# Analisando a correlaçao entre variaveis
colunas_corr = ["log_index_quadrado", "log_indice_x", "log_indice_y", "log_indice_de_orientaçao","log_soma_da_luminosidade","espessura_da_chapa_de_aço"]

matriz_corr = df_filtrado[colunas_corr].corr()

plt.figure(figsize=(6, 5))
sns.heatmap(matriz_corr, annot=True, cmap="coolwarm", fmt=".2f", linewidths=0.5)

plt.title("Matriz de Correlação", fontsize=14)
plt.tight_layout()
plt.show()

#Analisando a correlaçao entre entre as falhas
# Seleciona apenas as colunas de falhas
colunas_falhas = [col for col in df_filtrado.columns if 'falha' in col]

# Cria a matriz de correlação
matriz_corr = df_filtrado[colunas_falhas].corr()

# Plot da matriz de correlação
plt.figure(figsize=(8, 6))
sns.heatmap(matriz_corr, annot=True, cmap='coolwarm', vmin=0, vmax=1, linewidths=0.5)

plt.title('Matriz de Correlação entre Falhas', fontsize=14)
plt.tight_layout()
plt.show()

colunas_falhas = [col for col in df_filtrado.columns if col.startswith("falha_")]

colunas_corr = ["log_index_quadrado", "log_indice_x", "log_indice_y", "log_indice_de_orientaçao","log_soma_da_luminosidade","espessura_da_chapa_de_aço","tipo_do_aço_A300","tipo_do_aço_A400"]

# Garante que as colunas de falha estão como int
df_filtrado[colunas_falhas] = df_filtrado[colunas_falhas].astype(int)

# Seleciona apenas as colunas desejadas
df_corr = df_filtrado[colunas_corr + colunas_falhas]

# Calcula a matriz de correlação
corr_matrix = df_corr.corr()

# Plota a matriz de correlação
plt.figure(figsize=(14, 10))
sns.heatmap(corr_matrix, cmap='coolwarm', annot=True, fmt=".2f", vmin=-1, vmax=1)
plt.title('Matriz de Correlação - Variáveis Transformadas e Falhas')
plt.tight_layout()
plt.show()

# visualizaçao da correlaçao das variaveis e tipos de falhas em grafico de barras

correlacao = df_filtrado[colunas_corr + colunas_falhas].corr()

# Seleciona apenas as correlações entre variáveis e falhas (exclui correlação entre falhas)
cor_falhas = correlacao[colunas_falhas].loc[colunas_corr]

# Prepara os dados para o gráfico de barras
df_plot = cor_falhas.reset_index().melt(id_vars='index')
df_plot.columns = ['Variável', 'Falha', 'Correlação']

# Plota o gráfico de barras
plt.figure(figsize=(12, 6))
sns.barplot(data=df_plot, x='Falha', y='Correlação', hue='Variável')
plt.axhline(0, color='gray', linestyle='--')
plt.title('Correlação entre Variáveis Selecionadas e Falhas')
plt.xticks(rotation=45)
plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
plt.tight_layout()
plt.show()

df_filtrado.info()

# Colunas a desconsiderar por que tem muitos datos vazios, erros, informaçao fora do padrao etc. Eliminar (limpada y ajustada)
colunas_a_eliminar = [
    "soma_da_luminosidade", "index_quadrado", "indice_de_orientaçao","log_indice_de_orientaçao"
]

# Eliminar colunas do DataFrame
# Criar df_filtrado
colunas_existentes = [col for col in colunas_a_eliminar if col in df_filtrado.columns]

df_filtrado2 = df_filtrado.drop(columns=colunas_existentes)

df_filtrado2.info()

"""# **Estratégia para aprendizado binário (uma falha por vez)**

"""

#A falha_3 está numa posiçao intermediaria, boa escolha para iniciar teste
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report
from imblearn.over_sampling import SMOTE
from imblearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay


# 1. Definir X e y
X = df_filtrado2.drop(columns=['falha_3'])  # ou outra falha
y = df_filtrado2['falha_3']

# 2. Dividir em treino/teste
X_train, X_test, y_train, y_test = train_test_split(
    X, y, stratify=y, test_size=0.2, random_state=42
)

# 3. Pipeline com SMOTE + escalador + modelo
pipeline = Pipeline(steps=[
    ('scaler', StandardScaler()),
    ('smote', SMOTE(random_state=42)),
    ('clf', RandomForestClassifier(random_state=42, class_weight='balanced'))
])

# 4. Treinamento
pipeline.fit(X_train, y_train)

# 5. Avaliação
y_pred = pipeline.predict(X_test)
print(classification_report(y_test, y_pred))

#Matriz de confusão
ConfusionMatrixDisplay.from_predictions(y_test, y_pred, cmap='Blues')
plt.title("Matriz de Confusão - falha_3")
plt.grid(False)
plt.show()

# 6. Cross-validation (opcional)
scores = cross_val_score(pipeline, X, y, cv=5, scoring='f1')
print("F1 médio (5-fold CV):", scores.mean())

# Para priorizar o uso de SMOTE e balanceamento, mais crítico ao menos crítico falha_4 - falha_5 - falha_2 - falha_1
#As mais propensas a sofrer com o desbalanceamento, especialmente falha_4 e falha_5
#Teste falha_4
X = df_filtrado2.drop(columns=['falha_4'])  # ou outra falha
y = df_filtrado2['falha_4']


X_train, X_test, y_train, y_test = train_test_split(
    X, y, stratify=y, test_size=0.2, random_state=42
)

pipeline = Pipeline(steps=[
    ('scaler', StandardScaler()),
    ('smote', SMOTE(random_state=42)),
    ('clf', RandomForestClassifier(random_state=42, class_weight='balanced'))
])

pipeline.fit(X_train, y_train)

y_pred = pipeline.predict(X_test)
print(classification_report(y_test, y_pred))

ConfusionMatrixDisplay.from_predictions(y_test, y_pred, cmap='Blues')
plt.title("Matriz de Confusão - falha_4")
plt.grid(False)
plt.show()

scores = cross_val_score(pipeline, X, y, cv=5, scoring='f1')
print("F1 médio (5-fold CV):", scores.mean())

# Teste falha_5
X = df_filtrado2.drop(columns=['falha_5'])
y = df_filtrado2['falha_5']


X_train, X_test, y_train, y_test = train_test_split(
    X, y, stratify=y, test_size=0.2, random_state=42
)


pipeline = Pipeline(steps=[
    ('scaler', StandardScaler()),
    ('smote', SMOTE(random_state=42)),
    ('clf', RandomForestClassifier(random_state=42, class_weight='balanced'))
])


pipeline.fit(X_train, y_train)


y_pred = pipeline.predict(X_test)
print(classification_report(y_test, y_pred))


ConfusionMatrixDisplay.from_predictions(y_test, y_pred, cmap='Blues')
plt.title("Matriz de Confusão - falha_5")
plt.grid(False)
plt.show()

scores = cross_val_score(pipeline, X, y, cv=5, scoring='f1')
print("F1 médio (5-fold CV):", scores.mean())

# Teste falha_2
X = df_filtrado2.drop(columns=['falha_2'])
y = df_filtrado2['falha_2']


X_train, X_test, y_train, y_test = train_test_split(
    X, y, stratify=y, test_size=0.2, random_state=42
)


pipeline = Pipeline(steps=[
    ('scaler', StandardScaler()),
    ('smote', SMOTE(random_state=42)),
    ('clf', RandomForestClassifier(random_state=42, class_weight='balanced'))
])


pipeline.fit(X_train, y_train)


y_pred = pipeline.predict(X_test)
print(classification_report(y_test, y_pred))


ConfusionMatrixDisplay.from_predictions(y_test, y_pred, cmap='Blues')
plt.title("Matriz de Confusão - falha_2")
plt.grid(False)
plt.show()


scores = cross_val_score(pipeline, X, y, cv=5, scoring='f1')
print("F1 médio (5-fold CV):", scores.mean())

#Teste falha_1
X = df_filtrado2.drop(columns=['falha_1'])
y = df_filtrado2['falha_1']


X_train, X_test, y_train, y_test = train_test_split(
    X, y, stratify=y, test_size=0.2, random_state=42
)

pipeline = Pipeline(steps=[
    ('scaler', StandardScaler()),
    ('smote', SMOTE(random_state=42)),
    ('clf', RandomForestClassifier(random_state=42, class_weight='balanced'))
])

pipeline.fit(X_train, y_train)


y_pred = pipeline.predict(X_test)
print(classification_report(y_test, y_pred))

ConfusionMatrixDisplay.from_predictions(y_test, y_pred, cmap='Blues')
plt.title("Matriz de Confusão - falha_1")
plt.grid(False)
plt.show()

scores = cross_val_score(pipeline, X, y, cv=5, scoring='f1')
print("F1 médio (5-fold CV):", scores.mean())

#Teste falha_6
X = df_filtrado2.drop(columns=['falha_6'])
y = df_filtrado2['falha_6']

X_train, X_test, y_train, y_test = train_test_split(
    X, y, stratify=y, test_size=0.2, random_state=42
)


pipeline = Pipeline(steps=[
    ('scaler', StandardScaler()),
    ('smote', SMOTE(random_state=42)),
    ('clf', RandomForestClassifier(random_state=42, class_weight='balanced'))
])


pipeline.fit(X_train, y_train)


y_pred = pipeline.predict(X_test)
print(classification_report(y_test, y_pred))


ConfusionMatrixDisplay.from_predictions(y_test, y_pred, cmap='Blues')
plt.title("Matriz de Confusão - falha_6")
plt.grid(False)
plt.show()


scores = cross_val_score(pipeline, X, y, cv=5, scoring='f1')
print("F1 médio (5-fold CV):", scores.mean())

#Teste falha_outros
X = df_filtrado2.drop(columns=['falha_outros'])
y = df_filtrado2['falha_outros']


X_train, X_test, y_train, y_test = train_test_split(
    X, y, stratify=y, test_size=0.2, random_state=42
)


pipeline = Pipeline(steps=[
    ('scaler', StandardScaler()),
    ('smote', SMOTE(random_state=42)),
    ('clf', RandomForestClassifier(random_state=42, class_weight='balanced'))
])


pipeline.fit(X_train, y_train)


y_pred = pipeline.predict(X_test)
print(classification_report(y_test, y_pred))


ConfusionMatrixDisplay.from_predictions(y_test, y_pred, cmap='Blues')
plt.title("Matriz de Confusão - falha_outros")
plt.grid(False)
plt.show()

scores = cross_val_score(pipeline, X, y, cv=5, scoring='f1')
print("F1 médio (5-fold CV):", scores.mean())

df_filtrado.to_csv('df_filtrado2.csv', index=False)